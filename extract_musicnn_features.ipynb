{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is for pre-extracting MusicNN features for faster training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from musicnn import extractor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing MusicNN Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: dataset/musiccaps-audio-trimmed-16k/-0Gj8-vB1q4.wav\n",
      "Computing spectrogram (w/ librosa) and tags (w/ tensorflow).. done!\n",
      "(5, 200)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "DIR_NAME = 'dataset/musiccaps-audio-trimmed-16k'\n",
    "filename = os.listdir(DIR_NAME)[0]\n",
    "print(\"Reading: \"+ DIR_NAME + \"/\" + filename)\n",
    "taggram, tags, features = extractor.extractor(DIR_NAME + \"/\" + filename, model='MTT_musicnn', extract_features=True, input_length=2)\n",
    "print(features['penultimate'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-definition of musicnn's extractor class\n",
    "import tensorflow as tf\n",
    "# disable eager mode for tf.v1 compatibility with tf.v2\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from musicnn import models\n",
    "from musicnn import configuration as config\n",
    "import librosa\n",
    "\n",
    "\n",
    "def batch_data(audio, sr, n_frames, overlap):\n",
    "    '''For an efficient computation, we split the full music spectrograms in patches of length n_frames with overlap.\n",
    "    INPUT\n",
    "    \n",
    "    - audio - Audio data extracted with librosa\n",
    "    Data format: numpy array\n",
    "    Example: a load of numbers\n",
    "    \n",
    "    - sr - Sample Rate extracted with librosa\n",
    "    Data format: Integer? idk\n",
    "    Example: 44000\n",
    "    \n",
    "    - n_frames: length (in frames) of the input spectrogram patches.\n",
    "    Data format: integer.\n",
    "    Example: 187\n",
    "        \n",
    "    - overlap: ammount of overlap (in frames) of the input spectrogram patches.\n",
    "    Note: Set it considering n_frames.\n",
    "    Data format: integer.\n",
    "    Example: 10\n",
    "    \n",
    "    OUTPUT\n",
    "    \n",
    "    - batch: batched audio representation. It returns spectrograms split in patches of length n_frames with overlap.\n",
    "    Data format: 3D np.array (batch, time, frequency)\n",
    "    \n",
    "    - audio_rep: raw audio representation (spectrogram).\n",
    "    Data format: 2D np.array (time, frequency)\n",
    "    '''\n",
    "\n",
    "    # compute the log-mel spectrogram with librosa\n",
    "    #audio = audio_data\n",
    "    audio_rep = librosa.feature.melspectrogram(y=audio, \n",
    "                                               sr=sr,\n",
    "                                               hop_length=config.FFT_HOP,\n",
    "                                               n_fft=config.FFT_SIZE,\n",
    "                                               n_mels=config.N_MELS).T\n",
    "    audio_rep = audio_rep.astype(np.float16)\n",
    "    audio_rep = np.log10(10000 * audio_rep + 1)\n",
    "\n",
    "    # batch it for an efficient computing\n",
    "    first = True\n",
    "    last_frame = audio_rep.shape[0] - n_frames + 1\n",
    "    # +1 is to include the last frame that range would not include\n",
    "    for time_stamp in range(0, last_frame, overlap):\n",
    "        patch = np.expand_dims(audio_rep[time_stamp : time_stamp + n_frames, : ], axis=0)\n",
    "        if first:\n",
    "            batch = patch\n",
    "            first = False\n",
    "        else:\n",
    "            batch = np.concatenate((batch, patch), axis=0)\n",
    "\n",
    "    return batch, audio_rep\n",
    "\n",
    "\n",
    "def extract_audio_features(audio, sr=16000, model='MTT_musicnn', input_length=2, input_overlap=False, extract_features=True):\n",
    "    '''Extract the taggram (the temporal evolution of tags) and features (intermediate representations of the model) of the music-clip in file_name with the selected model.\n",
    "    INPUT \n",
    "    - audio - Audio data extracted with librosa\n",
    "    Data format: numpy array\n",
    "    Example: no\n",
    "    \n",
    "    - sr - Sample Rate extracted with librosa\n",
    "    Data format: Integer? idk\n",
    "    Example: 44000\n",
    "    \n",
    "    - model: select a music audio tagging model.\n",
    "    Data format: string.\n",
    "    Options: 'MTT_musicnn', 'MTT_vgg', 'MSD_musicnn', 'MSD_musicnn_big' or 'MSD_vgg'.\n",
    "    MTT models are trained with the MagnaTagATune dataset.\n",
    "    MSD models are trained with the Million Song Dataset.\n",
    "    To know more about these models, check our musicnn / vgg examples, and the FAQs.\n",
    "    Important! 'MSD_musicnn_big' is only available if you install from source: python setup.py install.\n",
    "    - input_length: length (in seconds) of the input spectrogram patches. Set it small for real-time applications.\n",
    "    Note: This is the length of the data that is going to be fed to the model. In other words, this parameter defines the temporal resolution of the taggram.\n",
    "    Recommended value: 3, because the models were trained with 3 second inputs.\n",
    "    Observation: the vgg models do not allow for different input lengths. For this reason, the vgg models' input_length needs to be set to 3. However, musicnn models allow for different input lengths: see this jupyter notebook.\n",
    "    Data format: floating point number.\n",
    "    Example: 3.1\n",
    "    \n",
    "    - input_overlap: ammount of overlap (in seconds) of the input spectrogram patches.\n",
    "    Note: Set it considering the input_length.\n",
    "    Data format: floating point number.\n",
    "    Example: 1.0\n",
    "    \n",
    "    - extract_features: set it True for extracting the intermediate representations of the model.\n",
    "    Data format: boolean.\n",
    "    Options: False (for NOT extracting the features), True (for extracting the features).\n",
    "    OUTPUT\n",
    "    \n",
    "    - taggram: expresses the temporal evolution of the tags likelihood.\n",
    "    Data format: 2D np.ndarray (time, tags).\n",
    "    Example: see our basic / advanced examples.\n",
    "    \n",
    "    - tags: list of tags corresponding to the tag-indices of the taggram.\n",
    "    Data format: list.\n",
    "    Example: see our FAQs page for the complete tags list.\n",
    "    \n",
    "    - features: if extract_features = True, it outputs a dictionary containing the activations of the different layers the selected model has.\n",
    "    Data format: dictionary.\n",
    "    Keys (musicnn models): ['timbral', 'temporal', 'cnn1', 'cnn2', 'cnn3', 'mean_pool', 'max_pool', 'penultimate']\n",
    "    Keys (vgg models): ['pool1', 'pool2', 'pool3', 'pool4', 'pool5']\n",
    "    Example: see our musicnn and vgg examples.\n",
    "    '''\n",
    "    \n",
    "    # select model\n",
    "    if 'MTT' in model:\n",
    "        labels = config.MTT_LABELS\n",
    "    elif 'MSD' in model:\n",
    "        labels = config.MSD_LABELS\n",
    "    num_classes = len(labels)\n",
    "    \n",
    "    if 'vgg' in model and input_length != 3:\n",
    "        raise ValueError('Set input_length=3, the VGG models cannot handle different input lengths.')\n",
    "\n",
    "    # convert seconds to frames\n",
    "    n_frames = librosa.time_to_frames(input_length, sr=config.SR, n_fft=config.FFT_SIZE, hop_length=config.FFT_HOP) + 1\n",
    "    if not input_overlap:\n",
    "        overlap = n_frames\n",
    "    else:\n",
    "        overlap = librosa.time_to_frames(input_overlap, sr=config.SR, n_fft=config.FFT_SIZE, hop_length=config.FFT_HOP)\n",
    "\n",
    "    # tensorflow: define the model\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    with tf.name_scope('model'):\n",
    "        x = tf.compat.v1.placeholder(tf.float32, [None, n_frames, config.N_MELS])\n",
    "        is_training = tf.compat.v1.placeholder(tf.bool)\n",
    "        if 'vgg' in model:\n",
    "            y, pool1, pool2, pool3, pool4, pool5 = models.define_model(x, is_training, model, num_classes)\n",
    "        else:\n",
    "            y, timbral, temporal, cnn1, cnn2, cnn3, mean_pool, max_pool, penultimate = models.define_model(x, is_training, model, num_classes)\n",
    "        normalized_y = tf.nn.sigmoid(y)\n",
    "\n",
    "    # tensorflow: loading model\n",
    "    sess = tf.compat.v1.Session()\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    try:\n",
    "        saver.restore(sess, os.path.dirname(__file__)+'/'+model+'/') \n",
    "    except:\n",
    "        if model == 'MSD_musicnn_big':\n",
    "            raise ValueError('MSD_musicnn_big model is only available if you install from source: python setup.py install')\n",
    "        elif model == 'MSD_vgg':\n",
    "            raise ValueError('MSD_vgg model is still training... will be available soon! :)')\n",
    "\n",
    "    # batching data\n",
    "    # THIS IS THE ONLY POINT WHICH NEEDS TO BE MODIFIED, EVERYTHING ELSE SHOULD BE FINE AS-IS\n",
    "    # print('Computing spectrogram (w/ librosa) and tags (w/ tensorflow)..', end =\" \")\n",
    "    batch, spectrogram = batch_data(audio, sr, n_frames, overlap)\n",
    "\n",
    "    # tensorflow: extract features and tags\n",
    "    # ..first batch!\n",
    "    if extract_features:\n",
    "        if 'vgg' in model:\n",
    "            extract_vector = [normalized_y, pool1, pool2, pool3, pool4, pool5]\n",
    "        else:\n",
    "            extract_vector = [normalized_y, timbral, temporal, cnn1, cnn2, cnn3, mean_pool, max_pool, penultimate]\n",
    "    else:\n",
    "        extract_vector = [normalized_y]\n",
    "\n",
    "    tf_out = sess.run(extract_vector, \n",
    "                      feed_dict={x: batch[:config.BATCH_SIZE], \n",
    "                      is_training: False})\n",
    "\n",
    "    if extract_features:\n",
    "        if 'vgg' in model:\n",
    "            predicted_tags, pool1_, pool2_, pool3_, pool4_, pool5_ = tf_out\n",
    "            features = dict()\n",
    "            features['pool1'] = np.squeeze(pool1_)\n",
    "            features['pool2'] = np.squeeze(pool2_)\n",
    "            features['pool3'] = np.squeeze(pool3_)\n",
    "            features['pool4'] = np.squeeze(pool4_)\n",
    "            features['pool5'] = np.squeeze(pool5_)\n",
    "        else:\n",
    "            predicted_tags, timbral_, temporal_, cnn1_, cnn2_, cnn3_, mean_pool_, max_pool_, penultimate_ = tf_out\n",
    "            features = dict()\n",
    "            features['timbral'] = np.squeeze(timbral_)\n",
    "            features['temporal'] = np.squeeze(temporal_)\n",
    "            features['cnn1'] = np.squeeze(cnn1_)\n",
    "            features['cnn2'] = np.squeeze(cnn2_)\n",
    "            features['cnn3'] = np.squeeze(cnn3_)\n",
    "            features['mean_pool'] = mean_pool_\n",
    "            features['max_pool'] = max_pool_\n",
    "            features['penultimate'] = penultimate_\n",
    "    else:\n",
    "        predicted_tags = tf_out[0]\n",
    "\n",
    "    taggram = np.array(predicted_tags)\n",
    "\n",
    "\n",
    "    # ..rest of the batches!\n",
    "    for id_pointer in range(config.BATCH_SIZE, batch.shape[0], config.BATCH_SIZE):\n",
    "\n",
    "        tf_out = sess.run(extract_vector, \n",
    "                          feed_dict={x: batch[id_pointer:id_pointer+config.BATCH_SIZE], \n",
    "                          is_training: False})\n",
    "\n",
    "        if extract_features:\n",
    "            if 'vgg' in model:\n",
    "                predicted_tags, pool1_, pool2_, pool3_, pool4_, pool5_ = tf_out\n",
    "                features['pool1'] = np.concatenate((features['pool1'], np.squeeze(pool1_)), axis=0)\n",
    "                features['pool2'] = np.concatenate((features['pool2'], np.squeeze(pool2_)), axis=0)\n",
    "                features['pool3'] = np.concatenate((features['pool3'], np.squeeze(pool3_)), axis=0)\n",
    "                features['pool4'] = np.concatenate((features['pool4'], np.squeeze(pool4_)), axis=0)\n",
    "                features['pool5'] = np.concatenate((features['pool5'], np.squeeze(pool5_)), axis=0)\n",
    "            else:\n",
    "                predicted_tags, timbral_, temporal_, midend1_, midend2_, midend3_, mean_pool_, max_pool_, penultimate_ = tf_out\n",
    "                features['timbral'] = np.concatenate((features['timbral'], np.squeeze(timbral_)), axis=0)\n",
    "                features['temporal'] = np.concatenate((features['temporal'], np.squeeze(temporal_)), axis=0)\n",
    "                features['cnn1'] = np.concatenate((features['cnn1'], np.squeeze(cnn1_)), axis=0)\n",
    "                features['cnn2'] = np.concatenate((features['cnn2'], np.squeeze(cnn2_)), axis=0)\n",
    "                features['cnn3'] = np.concatenate((features['cnn3'], np.squeeze(cnn3_)), axis=0)\n",
    "                features['mean_pool'] = np.concatenate((features['mean_pool'], mean_pool_), axis=0)\n",
    "                features['max_pool'] = np.concatenate((features['max_pool'], max_pool_), axis=0)\n",
    "                features['penultimate'] = np.concatenate((features['penultimate'], penultimate_), axis=0)\n",
    "        else:\n",
    "            predicted_tags = tf_out[0]\n",
    "\n",
    "        taggram = np.concatenate((taggram, np.array(predicted_tags)), axis=0)\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "    if extract_features:\n",
    "        return taggram, labels, features\n",
    "    else:\n",
    "        return taggram, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features for 0 / 5473 clips\n"
     ]
    }
   ],
   "source": [
    "for i, filename in enumerate(os.listdir('dataset/musiccaps-audio-trimmed-16k/')):\n",
    "    if i % 100 == 0:\n",
    "        print(f'Extracted features for {i} / 5473 clips')\n",
    "    audio, _ = librosa.load(f'{DIR_NAME}/{filename}', sr=16000)\n",
    "    _, _, features = extract_audio_features(audio, model='MTT_musicnn', extract_features=True, input_length=2)\n",
    "    np.savetxt(f\"dataset/musicnn_features2/{filename[:-4]}.csv\", features['penultimate'], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
